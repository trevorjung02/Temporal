#!/bin/bash
#SBATCH --job-name=data-proc
#SBATCH --partition=gpu-rtx6k
#SBATCH --account=cse
#SBATCH --nodes=1
#SBATCH --cpus-per-task=8
#SBATCH --mem=32G
#SBATCH --gres=gpu:1
#SBATCH --time=12:00:00 
#SBATCH --mail-type=ALL
#SBATCH --mail-user=tjung2@uw.edu

# I use source to initialize conda into the right environment.
cat $0
echo "--------------------"

source ~/.bashrc
conda activate ckl

# python run.py --config configs/templama/training/t5_kadapters_yearly_2freeze.json -checkpoint_path outputs/wmtbaseline_full/epoch=0-f1_score=0.151-em_score=0.000.ckpt

# python run.py --config configs/wmt/training/t5_kadapters_yearly_2freeze.json -checkpoint_path outputs/wmtkadapter_2010_2freeze_158_128/epoch=8-f1_score=0.084-em_score=0.000.ckpt

python run.py --config configs/wmt/training/t5_kadapters_yearly_2freeze.json

# python preprocess_wmt_train_data_full.py--------------------
wandb: Currently logged in as: tjung2. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.1
wandb: Run data is saved locally in /mmfs1/gscratch/ark/tjung2/continual-knowledge-learning/wandb/run-20220928_001010-ejsfyl10
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run kadapter_2010
wandb: ‚≠êÔ∏è View project at https://wandb.ai/tjung2/temporal_questions
wandb: üöÄ View run at https://wandb.ai/tjung2/temporal_questions/runs/ejsfyl10
/mmfs1/gscratch/ark/tjung2/miniconda3/envs/ckl/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:360: UserWarning: Checkpoint directory outputs/wmtkadapter_2010_2freeze_158_128 exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at google/t5-small-ssm and are newly initialized: ['enc_kadapter.adapter.0.encoder.layer.1.DenseReluDense.wi_0.weight', 'enc_kadapter.adapter.1.encoder.layer.0.SelfAttention.v.weight', 'enc_kadapter.adapter.1.encoder.layer.0.SelfAttention.k.weight', 'enc_kadapter.adapter.2.encoder.layer.0.SelfAttention.k.weight', 'enc_kadapter.adapter.1.up_project.bias', 'enc_kadapter.adapter.1.encoder.layer.1.DenseReluDense.wo.weight', 'enc_kadapter.layer_norm.weight', 'enc_kadapter.adapter.0.encoder.layer.1.DenseReluDense.wo.weight', 'enc_kadapter.adapter.2.encoder.layer.1.layer_norm.weight', 'enc_kadapter.adapter.1.up_project.weight', 'enc_kadapter.adapter.2.encoder.layer.0.SelfAttention.v.weight', 'enc_kadapter.adapter.1.encoder.layer.0.SelfAttention.o.weight', 'enc_kadapter.adapter.2.encoder.layer.1.DenseReluDense.wi_1.weight', 'enc_kadapter.adapter.0.encoder.layer.0.SelfAttention.relative_attention_bias.weight', 'enc_kadapter.adapter.1.down_project.weight', 'enc_kadapter.adapter.0.down_project.weight', 'enc_kadapter.adapter.2.down_project.weight', 'enc_kadapter.adapter.0.encoder.layer.0.layer_norm.weight', 'enc_kadapter.adapter.1.encoder.layer.1.DenseReluDense.wi_1.weight', 'enc_kadapter.adapter.2.up_project.bias', 'enc_kadapter.adapter.0.encoder.layer.0.SelfAttention.o.weight', 'enc_kadapter.adapter.1.encoder.layer.0.SelfAttention.q.weight', 'enc_kadapter.adapter.1.encoder.layer.1.layer_norm.weight', 'enc_kadapter.adapter.0.encoder.layer.0.SelfAttention.k.weight', 'enc_kadapter.adapter.1.encoder.layer.1.DenseReluDense.wi_0.weight', 'enc_kadapter.adapter.2.down_project.bias', 'enc_kadapter.adapter.0.encoder.layer.1.DenseReluDense.wi_1.weight', 'enc_kadapter.pool.bias', 'enc_kadapter.adapter.2.encoder.layer.0.layer_norm.weight', 'enc_kadapter.adapter.1.encoder.layer.0.SelfAttention.relative_attention_bias.weight', 'enc_kadapter.adapter.0.encoder.layer.0.SelfAttention.v.weight', 'enc_kadapter.adapter.0.up_project.bias', 'enc_kadapter.adapter.0.down_project.bias', 'enc_kadapter.adapter.2.up_project.weight', 'enc_kadapter.adapter.1.encoder.layer.0.layer_norm.weight', 'enc_kadapter.adapter.2.encoder.layer.0.SelfAttention.q.weight', 'enc_kadapter.adapter.0.up_project.weight', 'enc_kadapter.adapter.2.encoder.layer.0.SelfAttention.relative_attention_bias.weight', 'enc_kadapter.adapter.1.down_project.bias', 'enc_kadapter.adapter.2.encoder.layer.0.SelfAttention.o.weight', 'enc_kadapter.adapter.0.encoder.layer.1.layer_norm.weight', 'enc_kadapter.pool.weight', 'enc_kadapter.adapter.2.encoder.layer.1.DenseReluDense.wi_0.weight', 'enc_kadapter.adapter.2.encoder.layer.1.DenseReluDense.wo.weight', 'enc_kadapter.adapter.0.encoder.layer.0.SelfAttention.q.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Set SLURM handle signals.

  | Name  | Type                       | Params
-----------------------------------------------------
0 | model | T5ForConditionalGeneration | 79.1 M
-----------------------------------------------------
2.2 M     Trainable params
77.0 M    Non-trainable params
79.1 M    Total params
316.510   Total estimated model params size (MB)
Namespace(accelerator='ddp', adam_epsilon=1e-08, adapter_config={'adapter_list': [1, 5, 8], 'adapter_hidden_size': 128, 'adapter_enc_dec': None, 'pool_size': None}, adapter_enc_dec=None, adapter_hidden_size=128, adapter_list=[1, 5, 8], check_validation_only=False, checkpoint_dir=None, checkpoint_path='', dataset='wmt', dataset_version='2010', early_stop_callback=False, eval_batch_size=128, find_lr=False, freeze_embeds=False, freeze_encoder=False, freeze_level=2, learning_rate=0.001, max_grad_norm=0.5, max_input_length=350, max_output_length=50, method='kadapter', mode='pretrain', model_name_or_path='google/t5-small-ssm', n_gpu=1, n_test=-1, n_train=-1, n_val=-1, num_train_epochs=2, num_workers=4, opt_level='O1', output_dir='outputs/wmtkadapter_2010_2freeze_158_128', output_log=None, pool_size=None, prefix=True, resume_from_checkpoint=None, seed=42, split=0, split_num=1, t5_learning_rate=None, tokenizer_name_or_path='google/t5-small-ssm', train_batch_size=128, use_deepspeed=False, use_lr_scheduling=True, val_check_interval=500, val_data='2010', wandb_log=True, warmup_steps=0, weight_decay=0.0)
T5Config {
  "_name_or_path": "google/t5-small-ssm",
  "adapter_enc_dec": null,
  "adapter_hidden_size": 128,
  "adapter_list": [
    1,
    5,
    8
  ],
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "d_ff": 1024,
  "d_kv": 64,
  "d_model": 512,
  "decoder_start_token_id": 0,
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "num_decoder_layers": 8,
  "num_heads": 6,
  "num_layers": 8,
  "output_past": true,
  "pad_token_id": 0,
  "pool_size": null,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "transformers_version": "4.12.3",
  "use_cache": false,
  "vocab_size": 32128
}

hparams.learning_rate = 0.001
split is 0
Length of dataset retrieving is.. 500000
Index(['id', 'date', 'input', 'output'], dtype='object')
Validation sanity check: 0it [00:00, ?it/s]split is 0
Length of dataset retrieving is.. 32000
Index(['id', 'date', 'input', 'output'], dtype='object')
Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]Validation sanity check:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:19<00:19, 19.06s/it]Validation sanity check: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:37<00:00, 18.81s/it]                                                                      split is 0
Length of dataset retrieving is.. 500000
Index(['id', 'date', 'input', 'output'], dtype='object')
Training: 0it [00:00, ?it/s]Training:   0%|          | 0/5656 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/5656 [00:00<?, ?it/s] [W reducer.cpp:1158] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Epoch 0:   0%|          | 1/5656 [00:01<2:35:40,  1.65s/it]Epoch 0:   0%|          | 1/5656 [00:01<2:35:44,  1.65s/it, loss=13.6, v_num=yl10, em_score=0.000, f1_score=0.000]Epoch 0:   0%|          | 2/5656 [00:02<1:56:27,  1.24s/it, loss=13.6, v_num=yl10, em_score=0.000, f1_score=0.000]Epoch 0:   0%|          | 2/5656 [00:02<1:56:29,  1.24s/it, loss=13.6, v_num=yl10, em_score=0.000, f1_score=0.000]Epoch 0:   0%|          | 3/5656 [00:03<1:43:23,  1.10s/it, loss=13.6, v_num=yl10, em_score=0.000, f1_score=0.000]Epoch 0:   0%|          | 3/5656 [00:03<1:43:24,  1.10s/it, loss=13.5, v_num=yl10, em_score=0.000, f1_score=0.000]Epoch 0:   0%|          | 4/5656 [00:04<1:36:59,  1.03s/it, loss=13.5, v_num=yl10, em_score=0.000, f1_score=0.000]Epoch 0:   0%|          | 4/5656 [00:04<1:37:00,  1.03s/it, loss=13.3, v_num=yl10, em_score=0.000, f1_score=0.000]Epoch 0:   0%|          | 5/5656 [00:04<1:33:08,  1.01it/s, loss=13.3, v_num=yl10, em_score=0.000, f1_score=0.000]Epoch 0:   0%|          | 5/5656 [00:04<1:33:08,  1.01it/s, loss=13.2, v_num=yl10, em_score=0.000, f1_score=0.000]Epoch 0:   0%|          | 6/5656 [00:05<1:30:32,  1.04it/s, loss=13.2, v_num=yl10, em_score=0.000, f1_score=0.000]Epoch 0:   0%|          | 6/5656 [00:05<1:30:33,  1.04it/s, loss=13, v_num=yl10, em_score=0.000, f1_score=0.000]  Epoch 0:   0%|          | 7/5656 [00:06<1:28:42,  1.06it/s, loss=13, v_num=yl10, em_score=0.000, f1_score=0.000]Epoch 0:   0%|          | 7/5656 [00:06<1:28:42,  1.06it/s, loss=12.9, v_num=yl10, em_score=0.000, f1_score=0.000]Epoch 0:   0%|          | 8/5656 [00:07<1:27:16,  1.08it/s, loss=12.9, v_num=yl10, em_score=0.000, f1_score=0.000]Epoch 0:   0%|          | 8/5656 [00:07<1:27:17,  1.08it/s, loss=12.8, v_num=yl10, em_score=0.000, f1_score=0.000]Epoch 0:   0%|          | 9/5656 [00:08<1:26:11,  1.09it/s, loss=12.8, v_num=yl10, em_score=0.000, f1_score=0.000]Epoch 0:   0%|          | 9/5656 [00:08<1:26:12,  1.09it/s, loss=12.7, v_num=yl10, em_score=0.000, f1_score=0.000]Epoch 0:   0%|          | 10/5656 [00:09<1:25:18,  1.10it/s, loss=12.7, v_num=yl10, em_score=0.000, f1_score=0.000]Epoch 0:   0%|          | 10/5656 [00:09<1:25:18,  1.10it/s, loss=12.6, v_num=yl10, em_score=0.000, f1_score=0.000]Epoch 0:   0%|          | 11/5656 [00:09<1:24:36,  1.11it/s, loss=12.6, v_num=yl10, em_score=0.000, f1_score=0.000]Epoch 0:   0%|          | 11/5656 [00:09<1:24:37,  1.11it/s, loss=12.5, v_num=yl10, em_score=0.000, f1_score=0.000]Epoch 0:   0%|          | 12/5656 [00:10<1:24:00,  1.12it/s, loss=12.5, v_num=yl10, em_score=0.000, f1_score=0.000]Epoch 0:   0%|          | 12/5656 [00:10<1:24:00,  1.12it/s, loss=12.4, v_num=yl10, em_score=0.000, f1_score=0.000]Epoch 0:   0%|          | 13/5656 [00:11<1:23:28,  1.13it/s, loss=12.4, v_num=yl10, em_score=0.000, f1_score=0.000]Epoch 0:   0%|          | 13/5656 [00:11<1:23:28,  1.13it/s, loss=12.3, v_num=yl10, em_score=0.000, f1_score=0.000]Epoch 0:   0%|          | 14/5656 [00:12<1:23:01,  1.13it/s, loss=12.3, v_num=yl10, em_score=0.000, f1_score=0.000]Epoch 0:   0%|          | 14/5656 [00:12<1:23:01,  1.13it/s, loss=12.2, v_num=yl10, em_score=0.000, f1_score=0.000]Epoch 0:   0%|          | 15/5656 [00:13<1:22:38,  1.14it/s, loss=12.2, v_num=yl10, em_score=0.000, f1_score=0.000]Epoch 0:   0%|          | 15/5656 [00:13<1:22:38,  1.14it/s, loss=12.1, v_num=yl10, em_score=0.000, f1_score=0.000]Epoch 0:   0%|          | 16/5656 [00:14<1:22:17,  1.14it/s, loss=12.1, v_num=yl10, em_score=0.000, f1_score=0.000]Epoch 0:   0%|          | 16/5656 [00:14<1:22:17,  1.14it/s, loss=12, v_num=yl10, em_score=0.000, f1_score=0.000]  slurmstepd: error: *** JOB 6266546 ON g3022 CANCELLED AT 2022-09-28T00:11:36 ***
