#!/bin/bash
#SBATCH --job-name=data-proc
#SBATCH --partition=gpu-rtx6k
#SBATCH --account=cse
#SBATCH --nodes=1
#SBATCH --cpus-per-task=8
#SBATCH --mem=32G
#SBATCH --gres=gpu:1
#SBATCH --time=12:00:00 
#SBATCH --mail-type=ALL
#SBATCH --mail-user=tjung2@uw.edu

# I use source to initialize conda into the right environment.
cat $0
echo "--------------------"

source ~/.bashrc
conda activate ckl

# python run.py --config configs/templama/training/t5_kadapters_yearly_2freeze.json -checkpoint_path outputs/wmtbaseline_full/epoch=0-f1_score=0.151-em_score=0.000.ckpt

# python run.py --config configs/wmt/training/t5_kadapters_yearly_2freeze.json -checkpoint_path outputs/wmtkadapter_2010_2freeze_158_128/epoch=8-f1_score=0.084-em_score=0.000.ckpt

# python preprocess_wmt_train_data.py 2016
# python preprocess_wmt_train_data.py 2017
# python preprocess_wmt_train_data.py 2018

# python run.py --config configs/templama/training/t5_kadapters_soft_full.json -checkpoint_path outputs/wmtkadapter_2010_2freeze_158_128/epoch=8-f1_score=0.084-em_score=0.000.ckpt

# python run.py --config configs/templama/evaluation/t5_baseline_full.json -val_data 2010

# python run.py --config configs/templama/evaluation/t5_kadapters_soft_full.json -val_data 2010

python test.py
--------------------
Read csv: 5.794473545000001 seconds
decode sentences: 1.5883222470000007 seconds
Create sentences: 0.5144233759999999 seconds
Run spacy on sentences: 0.6050941480000009 seconds
166.526% of sentences have at least one named entity
