#!/bin/bash
#SBATCH --job-name=data-proc
#SBATCH --partition=gpu-2080ti
#SBATCH --account=ark
#SBATCH --nodes=1
#SBATCH --cpus-per-task=8
#SBATCH --mem=32G
#SBATCH --gres=gpu:1
#SBATCH --time=12:00:00 
#SBATCH --mail-type=ALL
#SBATCH --mail-user=tjung2@uw.edu

# I use source to initialize conda into the right environment.
cat $0
echo "--------------------"

source ~/.bashrc
conda activate ckl

# python run.py --config configs/templama/training/t5_kadapters_yearly_2freeze.json -checkpoint_path outputs/wmtbaseline_full/epoch=0-f1_score=0.151-em_score=0.000.ckpt

# python run.py --config configs/wmt/training/t5_kadapters_yearly_2freeze.json -checkpoint_path outputs/wmtkadapter_2010_2freeze_158_128/epoch=8-f1_score=0.084-em_score=0.000.ckpt

# python preprocess_wmt_train_data.py 2016
# python preprocess_wmt_train_data.py 2017
# python preprocess_wmt_train_data.py 2018

# python run.py --config configs/templama/training/t5_kadapters_soft_full.json -checkpoint_path outputs/wmtkadapter_2010_2freeze_158_128/epoch=8-f1_score=0.084-em_score=0.000.ckpt

# python run.py --config configs/templama/evaluation/t5_baseline_full.json -val_data 2010

# python run.py --config configs/templama/evaluation/t5_kadapters_soft_full.json -val_data 2010

python test.py
--------------------
Read csv: 5.579015636 seconds
decode sentences: 1.535199781000001 seconds
Create sentences: 0.49973605799999987 seconds
Run spacy on sentences: 0.5897306969999985 seconds
167.569% of sentences have at least one named entity
