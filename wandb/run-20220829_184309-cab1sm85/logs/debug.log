2022-08-29 18:43:09,963 INFO    MainThread:45775 [wandb_setup.py:_flush():76] Configure stats pid to 45775
2022-08-29 18:43:09,963 INFO    MainThread:45775 [wandb_setup.py:_flush():76] Loading settings from /mmfs1/home/tjung2/.config/wandb/settings
2022-08-29 18:43:09,963 INFO    MainThread:45775 [wandb_setup.py:_flush():76] Loading settings from wandb/settings
2022-08-29 18:43:09,963 WARNING MainThread:45775 [wandb_setup.py:_flush():76] Unknown environment variable: WANDB_SERVICE
2022-08-29 18:43:09,964 INFO    MainThread:45775 [wandb_setup.py:_flush():76] Loading settings from environment variables: {'_require_service': 'True', 'entity': 'tjung2', 'project': 'temporal_questions', 'sweep_id': 'tuiw1njj', 'root_dir': '/mmfs1/gscratch/ark/tjung2/continual-knowledge-learning', 'run_id': 'cab1sm85', 'sweep_param_path': '/mmfs1/gscratch/ark/tjung2/continual-knowledge-learning/wandb/sweep-tuiw1njj/config-cab1sm85.yaml'}
2022-08-29 18:43:09,965 INFO    MainThread:45775 [wandb_setup.py:_flush():76] Inferring run settings from compute environment: {'program_relpath': 'run.py', 'program': 'run.py'}
2022-08-29 18:43:09,965 INFO    MainThread:45775 [wandb_init.py:_log_setup():461] Logging user logs to /mmfs1/gscratch/ark/tjung2/continual-knowledge-learning/wandb/run-20220829_184309-cab1sm85/logs/debug.log
2022-08-29 18:43:09,965 INFO    MainThread:45775 [wandb_init.py:_log_setup():462] Logging internal logs to /mmfs1/gscratch/ark/tjung2/continual-knowledge-learning/wandb/run-20220829_184309-cab1sm85/logs/debug-internal.log
2022-08-29 18:43:09,965 INFO    MainThread:45775 [wandb_init.py:init():495] calling init triggers
2022-08-29 18:43:09,965 INFO    MainThread:45775 [wandb_init.py:init():498] wandb.init called with sweep_config: {'learning_rate': 0.003, 't5_learning_rate': 0.0001}
config: {'output_dir': 'outputs/T5_small_templama(full)_lr.001_kadapters_soft_prefixed', 'dataset': 'templama', 'dataset_version': 'full', 'prefix': True, 'split_num': 1, 'split': 0, 'model_name_or_path': 'google/t5-small-ssm', 'method': 'kadapter_soft', 'freeze_level': 1, 'mode': 'pretrain', 'tokenizer_name_or_path': 'google/t5-small-ssm', 'max_input_length': 50, 'max_output_length': 25, 'freeze_encoder': False, 'freeze_embeds': False, 'learning_rate': 0.01, 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'warmup_steps': 0, 'train_batch_size': 64, 'eval_batch_size': 64, 'num_train_epochs': 20, 'n_gpu': 1, 'num_workers': 4, 'resume_from_checkpoint': None, 'use_lr_scheduling': True, 'val_check_interval': 1.0, 'n_val': -1, 'n_train': -1, 'n_test': -1, 'early_stop_callback': False, 'use_deepspeed': False, 'opt_level': 'O1', 'max_grad_norm': 0.5, 'seed': 42, 'check_validation_only': False, 'checkpoint_path': '', 'accelerator': 'ddp', 'output_log': None, 'wandb_log': True, 'adapter_config': {'adapter_hidden_size': 128, 'adapter_list': [1, 5, 8], 'pool_size': 3}, 't5_learning_rate': 0.0003}
2022-08-29 18:43:09,965 INFO    MainThread:45775 [wandb_init.py:init():548] starting backend
2022-08-29 18:43:09,965 INFO    MainThread:45775 [wandb_init.py:init():552] setting up manager
2022-08-29 18:43:09,968 INFO    MainThread:45775 [backend.py:_multiprocessing_setup():97] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
2022-08-29 18:43:09,986 INFO    MainThread:45775 [wandb_init.py:init():558] backend started and connected
2022-08-29 18:43:09,990 INFO    MainThread:45775 [wandb_run.py:_config_callback():1152] config_cb None None {'learning_rate': 0.003, 't5_learning_rate': 0.0001}
2022-08-29 18:43:09,993 INFO    MainThread:45775 [wandb_init.py:init():636] updated telemetry
2022-08-29 18:43:10,042 INFO    MainThread:45775 [wandb_init.py:init():667] communicating run to backend with 30 second timeout
2022-08-29 18:43:10,217 INFO    MainThread:45775 [wandb_run.py:_on_init():1982] communicating current version
2022-08-29 18:43:10,239 INFO    MainThread:45775 [wandb_run.py:_on_init():1986] got version response upgrade_message: "wandb version 0.13.2 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"

2022-08-29 18:43:10,239 INFO    MainThread:45775 [wandb_init.py:init():700] starting run threads in backend
2022-08-29 18:43:15,251 INFO    MainThread:45775 [wandb_run.py:_console_start():1962] atexit reg
2022-08-29 18:43:15,251 INFO    MainThread:45775 [wandb_run.py:_redirect():1820] redirect: SettingsConsole.WRAP_RAW
2022-08-29 18:43:15,251 INFO    MainThread:45775 [wandb_run.py:_redirect():1885] Wrapping output streams.
2022-08-29 18:43:15,251 INFO    MainThread:45775 [wandb_run.py:_redirect():1907] Redirects installed.
2022-08-29 18:43:15,252 INFO    MainThread:45775 [wandb_init.py:init():732] run started, returning control to user process
2022-08-29 18:43:23,401 INFO    MainThread:45775 [wandb_run.py:_config_callback():1152] config_cb None None {'output_dir': 'outputs/T5_small_templama(full)_lr.001_kadapters_soft_prefixed', 'dataset': 'templama', 'dataset_version': 'full', 'prefix': True, 'split_num': 1, 'split': 0, 'model_name_or_path': 'google/t5-small-ssm', 'method': 'kadapter_soft', 'freeze_level': 1, 'mode': 'pretrain', 'tokenizer_name_or_path': 'google/t5-small-ssm', 'max_input_length': 50, 'max_output_length': 25, 'freeze_encoder': False, 'freeze_embeds': False, 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'warmup_steps': 0, 'train_batch_size': 64, 'eval_batch_size': 64, 'num_train_epochs': 20, 'n_gpu': 1, 'num_workers': 4, 'resume_from_checkpoint': 'None', 'use_lr_scheduling': True, 'val_check_interval': 1.0, 'n_val': -1, 'n_train': -1, 'n_test': -1, 'early_stop_callback': False, 'use_deepspeed': False, 'opt_level': 'O1', 'max_grad_norm': 0.5, 'seed': 42, 'check_validation_only': False, 'checkpoint_path': '', 'accelerator': 'ddp', 'output_log': 'None', 'wandb_log': True, 'adapter_config/adapter_hidden_size': 128, 'adapter_config/adapter_list': [1, 5, 8], 'adapter_config/pool_size': 3}
